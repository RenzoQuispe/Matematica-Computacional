{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c605e34",
   "metadata": {},
   "source": [
    "## Algoritmos de Descenso \n",
    "\n",
    "### <u>Optimización</u>\n",
    "\n",
    "La **teoría de optimización** o **programación matemática** está constituida por un conjunto de resultados y métodos numéricos enfocados en **encontrar e identificar al mejor candidato** de entre una colección de alternativas, sin tener que enumerar y evaluar explícitamente todas ellas.  \n",
    "\n",
    "En general, un **problema de optimización** consiste en:\n",
    "\n",
    "- **Buscar valores** para determinadas variables,  \n",
    "- De forma que cumplan un **conjunto de requisitos** (restricciones), representados mediante **ecuaciones y/o inecuaciones algebraicas**,  \n",
    "- Y que proporcionen el **mejor valor posible** (máximo o mínimo) para una función,\n",
    "\n",
    "### <u>Planteamiento General de un Problema de Optimización</u>\n",
    "\n",
    "Un problema de **optimización** puede expresarse de forma general como:  \n",
    "\n",
    "$$\n",
    "\\max \\; \\text{o} \\; \\min \\; f(x)\n",
    "$$\n",
    "\n",
    "sujeto a:  \n",
    "\n",
    "$$\n",
    "g(x) \\leq 0, \\quad h(x) = 0\n",
    "$$\n",
    "\n",
    "#### Elementos de Optimización\n",
    "\n",
    "- **Función objetivo**: Expresión matemática que se desea maximizar o minimizar.  \n",
    "- **Variables independientes**: Parámetros o decisiones a determinar.  \n",
    "- **Restricciones**: Condiciones (igualdades o desigualdades) que deben cumplir las variables.  \n",
    "\n",
    "#### Tipos de Problemas de Optimización\n",
    "\n",
    "1. **Problemas de Programación Lineal (PL):**  \n",
    "   Aquellos en los que tanto la **función objetivo** como las **restricciones** son funciones lineales de las variables de decisión.  \n",
    "\n",
    "2. **Problemas de Programación No Lineal (PNL):**  \n",
    "   - **Con restricciones:** La función objetivo o alguna restricción es no lineal.  \n",
    "   - **Sin restricciones:** Solo se optimiza la función objetivo sin imponer condiciones adicionales.  \n",
    "\n",
    "### Clasificación según el tipo de problema\n",
    "\n",
    "1. **Optimización Continua:**  \n",
    "   Los valores de las variables de decisión pueden tomar **cualquier valor dentro de un intervalo** (conjunto continuo).  \n",
    "   - Ejemplo: Minimizar $f(x) = x^2$ con $x \\in \\mathbb{R}$.  \n",
    "\n",
    "2. **Optimización Discreta:**  \n",
    "   Las variables de decisión solo pueden asumir **valores enteros o de un conjunto finito**.  \n",
    "   - Ejemplo: Problema del **viajante** (TSP), donde se busca la ruta óptima visitando un conjunto de ciudades una sola vez.  \n",
    "\n",
    "#### Solución Factible\n",
    "\n",
    "Una **solución factible** $x$ es aquella que **cumple todas las restricciones** del problema.  \n",
    "\n",
    "#### Conjunto Factible\n",
    "\n",
    "El **conjunto factible** o **región factible** $\\Omega$ es el conjunto de **todas las soluciones factibles**.  \n",
    "\n",
    "#### Solución Óptima\n",
    "\n",
    "- Una solución $x_0$ es **óptima** si es un **mínimo global** y el objetivo del problema es **minimizar**.  \n",
    "- Una solución $x_0$ es **óptima** si es un **máximo global** y el objetivo del problema es **maximizar**.  \n",
    "\n",
    "#### Valor Óptimo\n",
    "\n",
    "Si $x_0$ es una solución óptima, entonces **$f(x_0)$ es el valor óptimo** del problema.  \n",
    "\n",
    "### <u>Teorema de Weierstrass</u>\n",
    "\n",
    "Sea $f(x)$ una **función continua** definida sobre un **conjunto compacto** $K$.  \n",
    "Entonces, el problema de optimización tiene al menos **una solución óptima**.  \n",
    "\n",
    "#### Propiedad\n",
    "\n",
    "El conjunto de **soluciones óptimas** de un problema de optimización es un **conjunto convexo**.  \n",
    "\n",
    "#### Problemas a Resolver\n",
    "\n",
    "1. ¿Cómo podemos determinar si un punto $x_0$ es o no la **solución óptima** de un problema de optimización?  \n",
    "2. Si $x$ no es el **punto óptimo**, entonces ¿cómo podemos encontrar una **solución óptima** $x_0$ utilizando la información de la función?  \n",
    "\n",
    "### <u>Programación no Lineal sin Restricciones</u>\n",
    "\n",
    "Sea $U \\subset \\mathbb{R}^n$ y una función $f : U \\to \\mathbb{R}$, el planteamiento general de un problema sin restricciones es:  \n",
    "\n",
    "**Minimización**  \n",
    "$$\n",
    "\\min f(x) \\quad \\text{sujeto a } x \\in U\n",
    "$$  \n",
    "\n",
    "Lo cual consiste en determinar:  \n",
    "$$\n",
    "x_0 \\in U \\quad \\text{tal que } f(x_0) \\leq f(x), \\quad \\forall x \\in U\n",
    "$$  \n",
    "\n",
    "**Maximización**  \n",
    "$$\n",
    "\\max f(x) \\quad \\text{sujeto a } x \\in U\n",
    "$$  \n",
    "\n",
    "Lo cual consiste en determinar:  \n",
    "$$\n",
    "x_0 \\in U \\quad \\text{tal que } f(x_0) \\geq f(x), \\quad \\forall x \\in U\n",
    "$$  \n",
    "\n",
    "### <u>Algoritmos de Descenso</u>\n",
    "\n",
    "- Son **algoritmos iterativos**: se inicia especificando un punto de partida y se genera una **serie de puntos**, cada uno calculado en función de los anteriores.  \n",
    "- A medida que se genera un nuevo punto, el valor de la función decrece.  \n",
    "- Idealmente, la sucesión de puntos generados por el algoritmo **converge** (en un número finito o infinito de pasos) hacia la **solución del problema original**.  \n",
    "\n",
    "### <u>Convergencia Global de Algoritmos de Descenso</u>\n",
    "\n",
    "Si se garantiza que para **puntos de partida arbitrarios** el algoritmo genera una sucesión de puntos que convergen a una solución, entonces se dice que el algoritmo es **globalmente convergente**.  \n",
    "\n",
    "#### Definición:\n",
    "\n",
    "Un **algoritmo** $A$ es una transformación definida en un espacio $X$ que asigna a todo punto $x \\in X$ un subconjunto de $X$.  \n",
    "\n",
    "El algoritmo $A$ iniciado en $x_1 \\in X$ genera la sucesión $(x_k)$ definida por:  \n",
    "\n",
    "$$\n",
    "x_{k+1} = A(x_k).\n",
    "$$\n",
    "\n",
    "### <u>Algoritmo de Descenso</u>\n",
    "\n",
    "#### Definición:\n",
    "\n",
    "Sea $T \\subset X$ un **conjunto solución** dado y sea $A$ un algoritmo en $X$.  \n",
    "\n",
    "Una función continua $f: X \\to \\mathbb{R}$ es una **función descendente** para $T$ y $A$ si satisface:  \n",
    "\n",
    "1. Si $x \\notin T$ e $y \\in A(x)$, entonces  \n",
    "   $$\n",
    "   f(y) < f(x).\n",
    "   $$  \n",
    "\n",
    "2. Si $x \\in T$ e $y \\in A(x)$, entonces  \n",
    "   $$\n",
    "   f(y) \\leq f(x).\n",
    "   $$  \n",
    "\n",
    "### <u>Criterios de Parada</u>\n",
    "\n",
    "Sea $\\epsilon > 0$ y $N \\in \\mathbb{Z}^+$.  \n",
    "Para detener el **algoritmo de descenso** podemos usar los siguientes criterios:  \n",
    "\n",
    "1.  \n",
    "   $$\n",
    "   \\|x_{k+N} - x_k\\| < \\epsilon\n",
    "   $$\n",
    "\n",
    "2.  \n",
    "   $$\n",
    "   \\frac{\\|x_{k+N} - x_k\\|}{\\|x_k\\|} < \\epsilon\n",
    "   $$\n",
    "\n",
    "3.  \n",
    "   $$\n",
    "   A(x_k) - A(x_{k+N}) < \\epsilon\n",
    "   $$\n",
    "\n",
    "4.  \n",
    "   $$\n",
    "   \\frac{A(x_k) - A(x_{k+N})}{|A(x_k)|} < \\epsilon\n",
    "   $$\n",
    "\n",
    "5.  \n",
    "   $$\n",
    "   A(x_k) - A(x^*), \\quad x^* \\in T\n",
    "   $$\n",
    "\n",
    "### <u>Convergencia</u>\n",
    "\n",
    "La **velocidad de convergencia** refleja la eficiencia del algoritmo.  \n",
    "\n",
    "Sea la sucesión $(x_k)$ de números reales que converge a $x^*$, asumiendo que $x_k \\neq x^*$ para todo $k$.  \n",
    "\n",
    "El **orden de convergencia** de la sucesión es el supremo no negativo de números $p$ que satisfacen:  \n",
    "\n",
    "$$\n",
    "\\lim_{k \\to +\\infty} \\frac{|x_{k+1} - x^*|}{|x_k - x^*|^p} = c < +\\infty\n",
    "$$\n",
    "\n",
    "### <u>Algoritmo de Descenso: Método de Newton</u>\n",
    "\n",
    "Sea $f : \\mathbb{R} \\to \\mathbb{R}$ una función de clase $C^2$.  \n",
    "Se puede construir una función cuadrática $g$ que concuerde con $f$ en $x_k$ hasta la segunda derivada, es decir:\n",
    "\n",
    "$$\n",
    "g(x) = f(x_k) + f'(x_k)(x - x_k) + \\tfrac{1}{2} f''(x_k)(x - x_k)^2\n",
    "$$\n",
    "\n",
    "Se puede estimar $x_{k+1}$ a partir del punto mínimo de $f$, hallando el punto en el que se anula la derivada de $g$:  \n",
    "\n",
    "$$\n",
    "0 = g'(x_{k+1}) = f'(x_k) + f''(x_k)(x_{k+1} - x_k)\n",
    "$$\n",
    "\n",
    "Por lo tanto:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}.\n",
    "$$\n",
    "\n",
    "\n",
    "Ahora, sea $f : \\mathbb{R}^n \\to \\mathbb{R}$.  \n",
    "Para minimizar una función $f$, este método se basa en aproximarla mediante la **expansión de Taylor**:\n",
    "\n",
    "$$\n",
    "f(x) = f(x_k) + \\nabla f(x_k)^T (x - x_k) + \\tfrac{1}{2} (x - x_k)^T H(x_k) (x - x_k)\n",
    "$$\n",
    "\n",
    "Supongamos que $H(x_k)$ (la matriz Hessiana) es **definida positiva**, entonces se alcanza un **mínimo local no restringido** y se cumple:\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\nabla f(x_k) + H(x_k)(x - x_k) = 0,\n",
    "$$\n",
    "\n",
    "siendo $H(x_k)$ **invertible**.  \n",
    "\n",
    "El valor de minimización de $x$ está dado por:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - H(x_k)^{-1} \\nabla f(x_k).\n",
    "$$\n",
    "\n",
    "### <u>Convergencia</u>\n",
    "\n",
    "Sea $f'(x) = g(x)$, entonces:\n",
    "\n",
    "$$\n",
    "x_{k+1} - x^{*} = x_k - x^{*} - \\frac{g(x_k) - g(x^{*})}{g'(x_k)}\n",
    "$$\n",
    "\n",
    "Reordenando:\n",
    "\n",
    "$$\n",
    "x_{k+1} - x^{*} = -\\frac{g(x_k) - g(x^{*}) - g'(x_k)(x_k - x^{*})}{g'(x_k)}\n",
    "$$\n",
    "\n",
    "Aplicando el desarrollo de Taylor:\n",
    "\n",
    "$$\n",
    "x_{k+1} - x^{*} = -\\frac{g''(x_k)}{2g'(x_k)} (x_k - x^{*})^2\n",
    "$$\n",
    "\n",
    "Dado que cerca de $x^{*}$ las funciones $g'(x)$ y $g''(x)$ están **acotadas**, se tiene:\n",
    "\n",
    "$$\n",
    "|x_{k+1} - x^{*}| \\leq K |x_k - x^{*}|^2\n",
    "$$\n",
    "\n",
    "Por lo tanto, el **método de Newton** tiene **orden de convergencia cuadrática (orden 2)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304369b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def newton_multivariable(func, vars, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Método de Newton en R^n\n",
    "    \"\"\"\n",
    "    # Gradiente y Hessiano simbólicos\n",
    "    grad_f = [sp.diff(func, v) for v in vars]\n",
    "    H_f = sp.hessian(func, vars)\n",
    "\n",
    "    # Convertimos a funciones numéricas\n",
    "    grad_f_lambd = sp.lambdify(vars, grad_f, \"numpy\")\n",
    "    H_f_lambd = sp.lambdify(vars, H_f, \"numpy\")\n",
    "    f_lambd = sp.lambdify(vars, func, \"numpy\")\n",
    "\n",
    "    xk = np.array(x0, dtype=float)\n",
    "\n",
    "    print(\"Iteración | xk                | f(xk)       | ||grad||\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        grad_val = np.array(grad_f_lambd(*xk), dtype=float).flatten()\n",
    "        H_val = np.array(H_f_lambd(*xk), dtype=float)\n",
    "        f_val = f_lambd(*xk)\n",
    "        norm_grad = np.linalg.norm(grad_val)\n",
    "\n",
    "        print(f\"{k:9d} | {xk} | {f_val:11.6f} | {norm_grad:8.2e}\")\n",
    "\n",
    "        if norm_grad < tol:\n",
    "            print(\"Convergencia alcanzada :D\")\n",
    "            return xk, f_val, k\n",
    "\n",
    "        # Paso de Newton: resolver H p = grad\n",
    "        try:\n",
    "            p = np.linalg.solve(H_val, grad_val)\n",
    "        except np.linalg.LinAlgError:\n",
    "            raise ValueError(\"La Hessiana no es invertible en xk\")\n",
    "\n",
    "        xk = xk - p\n",
    "\n",
    "    print(\"Se alcanzó el máximo de iteraciones:(\")\n",
    "    return xk, f_lambd(*xk), max_iter\n",
    "\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = (x-1)**3 + (y-2)**2\n",
    "vars = [x, y]\n",
    "x0 = [0, 0]\n",
    "\n",
    "sol, fval, iters = newton_multivariable(f, vars, x0)\n",
    "print(\"\\nResultado final:\")\n",
    "print(\"x* =\", sol)\n",
    "print(\"f(x*) =\", fval)\n",
    "print(\"Iteraciones =\", iters)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
